# -*- coding: utf-8 -*-
"""SentimentAnalysis.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PdwglBAtxUQKmI7jdVeA7xkQD7ZZIEx0
"""

import pandas as pd
import numpy as np

df = pd.read_csv("Reviews.csv", on_bad_lines='skip', engine='python')

df = df[['Text','Score']]

df.dropna(inplace=True)

# Convert 'Score' column to numeric, coercing errors to NaN
df['Score'] = pd.to_numeric(df['Score'], errors='coerce')
# After converting, drop any rows where conversion resulted in NaN
df.dropna(subset=['Score'], inplace=True)

df['Score'] = df['Score'] - 1

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Score'], test_size = 0.2, random_state=42, stratify=df['Score'])

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 20000
max_len = 200

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq  = tokenizer.texts_to_sequences(X_test)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')
X_test_pad  = pad_sequences(X_test_seq, maxlen=max_len, padding='post')

from tensorflow.keras.utils import to_categorical

y_train_cat = to_categorical(y_train, num_classes=5)
y_test_cat  = to_categorical(y_test, num_classes=5)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout

model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),
    GlobalAveragePooling1D(),

    Dense(128, activation='relu'),
    Dropout(0.3),

    Dense(64, activation='relu'),
    Dropout(0.3),

    Dense(5, activation='softmax')  # 5 classes
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

classes = np.unique(y_train)
class_weights = compute_class_weight(class_weight='balanced',
                                     classes=classes,
                                     y=y_train)
class_weight_dict = dict(zip(classes, class_weights))
print("Class weights:", class_weight_dict)

# Rebuild / recompile model (if needed) but change loss to sparse_categorical_crossentropy:
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Fit using integer labels (no to_categorical needed for y_train/y_test)
history = model.fit(X_train_pad, y_train.values,
                    validation_split=0.1,
                    epochs=5,
                    batch_size=128,
                    class_weight=class_weight_dict,
                    verbose=1)

history = model.fit(
    X_train_pad, y_train_cat,
    validation_split=0.1,
    epochs=5,
    batch_size=128,
    verbose=1
)

loss, acc = model.evaluate(X_test_pad, y_test.values)
print(f"Test Accuracy: {acc:.4f}")



new_review = ["This is a bad product"]

seq = tokenizer.texts_to_sequences(new_review)
pad = pad_sequences(seq, maxlen=max_len)

pred = model.predict(pad)
predicted_class = pred.argmax(axis=1)[0]

print("Predicted Rating:", predicted_class + 1)  # convert back to 1â€“5

# 1) Check label distribution (train + test)
print("Full dataset class counts:\n", df['Score'].value_counts().sort_index())

print("\nTrain class counts:\n", y_train.value_counts().sort_index())
print("\nTest class counts:\n", y_test.value_counts().sort_index())

# 2) Ensure labels are integers 0..4
print("\nLabel dtype and unique values:", y_train.dtype, y_train.unique()[:10])

